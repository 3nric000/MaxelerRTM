#LyX 2.0 created this file. For more info see http://www.lyx.org/
\lyxformat 413
\begin_document
\begin_header
\textclass article
\use_default_options true
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding global
\font_roman default
\font_sans default
\font_typewriter default
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100
\font_tt_scale 100

\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_amsmath 1
\use_esint 1
\use_mhchem 1
\use_mathdots 1
\cite_engine basic
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\use_refstyle 1
\index Index
\shortcut idx
\color #008000
\end_index
\leftmargin 2.5cm
\topmargin 2.5cm
\rightmargin 2.5cm
\bottommargin 2.5cm
\headheight 2.5cm
\headsep 2.5cm
\footskip 2.5cm
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\quotes_language english
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Title
HPPS Project: RTM - Maxeler
\end_layout

\begin_layout Author
Enrico Deiana & Emanuele Del Sozzo
\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Standard
\begin_inset CommandInset toc
LatexCommand tableofcontents

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Newpage pagebreak
\end_inset


\end_layout

\begin_layout Section
Abstract
\end_layout

\begin_layout Standard
The objective of this project is to accelerate in hardware the RTM application
 using the Maxeler platform.
 First of all, it was necessary to analyze the C code to find out the critical
 parts.
 This analysis was done both by looking at the C code and by using profiling
 tools (part 3).
 Meanwhile, we started to study how the Maxeler platform works by reading
 the manuals and looking at the examples inside Maxide.
 Then, we decided which function was the most candidate to be implemented
 in hardware.
 During the implementation, we followed several approaches (part 4), from
 trivial to more complex ones.
 In the end, only with the last approach we were able to synthetize the
 kernel on the FPGA board and examine the result of hardware acceleration.
\end_layout

\begin_layout Section
RTM
\end_layout

\begin_layout Standard
Reverse Time Migration (RTM) is the current state-of-the-art in seismic
 imaging.
 It aims to construct an image of the subsurface from reflection seismics
 recordings.
\end_layout

\begin_layout Standard
It is mainly made up of two processes: 1) seismic forward modelling of a
 wave field originating from the source position, 2) reverse-time modelling
 of the shot-record, where the time samples are put into the modelling scheme.
\end_layout

\begin_layout Standard
The idea behind this procedure is that sufficiently strong correlation amplitude
s can only be expected at subsurface locations where a reflection occurs.
 Occurrence of a reflection at a certain location means that there is a
 down-going wave field from the source side and an up-going wave field towards
 the receivers at the same time.
\end_layout

\begin_layout Standard
The implementation of the RTM that has been provided to us is composed by
 three C source files 
\emph on
cpu_main.c
\emph default
, 
\emph on
cpu_constructors.c
\emph default
, 
\emph on
cpu_simulation.c
\emph default
 and two header files 
\emph on
cpu_constructors.h
\emph default
 and 
\emph on
cpu_simulation.h
\emph default
 which declare the functions present in their respective source file.
\end_layout

\begin_layout Description
cpu_main.c contains just the main function which is in charge to call all
 the subroutines that: initializes the random seed for the random field
 creation and all the needed parameters, constructs the model and all the
 necessary structures for the computation, simulates the model and store
 the results.
\end_layout

\begin_layout Description
cpu_constructors.c implements all the functions responsible for allocation
 and initialization of all global variables and required data structures.
 Parsing of the parameter file, earth model file when required, and receiver
 input data file when available.
 Construction of structures required during simulation.
\end_layout

\begin_layout Description
cpu_simulation.c includes all the functions responsible for wave equation,
 injection of the source signal, saving the acquired receiver signal, propagatin
g the source/receiver, removing the source signal from the receiver acquisition
 data, correlating the wavefields to create the several subsurface images,
 reinjecting the receiver data during back propagation, backpropagating
 both the receiver and the source and correlating the wavefields.
\end_layout

\begin_layout Standard
Several files containing the parameters needed for the creation of the model
 and for the simulation are provided in the 
\emph on
tests
\emph default
 directory.
 These are: 
\emph on
rtm_parameters_small.txt
\emph default
, 
\emph on
rtm_parameters_medium.txt
\emph default
, 
\emph on
rtm_parameters_large.txt
\emph default
, 
\emph on
marmousi_0.txt
\emph default
, 
\emph on
marmousi_32.txt 
\emph default
.
 For our tests we have mainly used the small one.
 More informations on the structure of the program are provided into the
 README.txt file.
 
\end_layout

\begin_layout Standard
As we will see in the profiling section, what we are particulary intrested
 in are two functions in whom the program spends most of its time: the 
\emph on
do_step 
\emph default
and 
\emph on
do_step_damping 
\emph default
which are in the 
\emph on
cpu_simulation.c 
\emph default
file and are in charge of propagating the wavefield.
\end_layout

\begin_layout Section
Profiling
\end_layout

\begin_layout Standard
First of all we looked at the code to identify the most critical parts of
 the program.
 We immediately detected two functions, 
\emph on
do_step 
\emph default
and
\emph on
 do_step_damping
\emph default
, with a very heavy computation consisting in three nested loops.
 But for a better application analysis, in order to know how much time the
 program spends inside its differents subroutines, we profiled the RTM applicati
on (with the
\emph on
 rtm_parameters_small.txt
\emph default
 input file) with three well known profiling tools: 
\emph on
callgrind
\emph default
, 
\emph on
gprof
\emph default
, 
\emph on
oprofile
\emph default
.
 However the most interesting results are returned by 
\emph on
callgrind
\emph default
 and 
\emph on
gprof
\emph default
, while 
\emph on
oprofile
\emph default
 results are rather similar to the 
\emph on
callgrind
\emph default
 ones (moreover with some lack of informations).
\end_layout

\begin_layout Subsection*
Callgrind
\end_layout

\begin_layout Standard
Callgrind is part of valgrind's tool suite, it records the call history
 among functions in a program's run as a call-graph.
 The main advantage of callgrind is that we don't need to recompile the
 application, but this tool adds a huge amount of overhead during the program
 execution (that took approximately 15 times more than the normal execution).
\end_layout

\begin_layout Standard
We have profiled the RTM application using the small data:
\end_layout

\begin_layout Standard
\begin_inset listings
inline false
status open

\begin_layout Plain Layout

$ valgrind --tool=callgrind rtm-cpu ../../tests/rtm_parameters_small.txt
\end_layout

\end_inset


\end_layout

\begin_layout Standard
What we found out is that RTM spends most of the time (63.89%) inside 
\emph on
do_step_damping
\emph default
 propagating the wavefield of one step in time each time it is called.
 The callers are 
\emph on
prop_source 
\emph default
and 
\emph on
migrate_shot
\emph default
, the first one propagates the source forwards, the second one back propagates
 the source and the receiver and correlates the wavefields, both call 
\emph on
do_step_damping 
\emph default
1000 times, so it is called 2000 times overall.
 Below is shown the callee graph for this function:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/enrico/Other/repos/MaxelerRTM/report/dostepdampingmin.png
	scale 60

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
do_step_damping
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Another function, moreover very similar to the 
\emph on
do_step_damping
\emph default
, is 
\emph on
do_step,
\emph default
 where the program spends 15.17% of its time.
 Even this time the callers are 
\emph on
prop_source 
\emph default
and 
\emph on
migrate_shot
\emph default
 that calls it again 1000 times each.
 The callee graph is this one:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/enrico/Other/repos/MaxelerRTM/report/dostepmin.png
	scale 60

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
do_step
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Finally 
\emph on
image_it
\emph default
 is the last more onerous function, it applies the imaging condition after
 the backpropagation of the source and the receiver.
 RTM spends here, according to callgrind, the 20.22% of its time.
 The caller is 
\emph on
migrate_shot
\emph default
 that calls 
\emph on
image_it
\emph default
 1000 times.
 The callee graph is:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/enrico/Other/repos/MaxelerRTM/report/imageitmin.png
	scale 60

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
image_it
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The overall meaningful calls done by 
\emph on
main
\emph default
 are shown here:
\end_layout

\begin_layout Standard
\begin_inset Float figure
placement H
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename /home/enrico/Other/repos/MaxelerRTM/report/mainmin.png
	scale 60

\end_inset


\begin_inset Caption

\begin_layout Plain Layout
main
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Subsection*
Gprof
\end_layout

\begin_layout Standard
Gprof is the GNU profiling tools.
 It needs to recompile the application for doing the profiling with the
 
\emph on
-pg
\emph default
 option, so we have modified the 
\emph on
MakeFile 
\emph default
and built again the program, but the needed overhead for the profiling part
 is quite small so that it almost doesn't affect the program execution.
\end_layout

\begin_layout Standard
Respect to callgrind there are some mismatches, especially for 
\emph on
do_step
\emph default
 and 
\emph on
image_it
\emph default
, where gprof states that the program spends 17.7% and 16.9% respectively
 (instead of 15.17% and 20.22% stated by callgrind).
 While for the 
\emph on
do_step_damping
\emph default
 function the amount of time spent is quite similar (64.3% wrt 63.89%).
 The number of calls and the caller is, obviously, the same stated before
 in the callgrind part (2000
\begin_inset Formula $\times$
\end_inset


\emph on
 do_step_damping 
\emph default
and 
\emph on
do_step
\emph default
, 1000
\begin_inset Formula $\times$
\end_inset

 
\emph on
image_it
\emph default
).
\end_layout

\begin_layout Standard
More informations about this profiling done with gprof can be found in the
 
\begin_inset Quotes eld
\end_inset

analysis.txt
\begin_inset Quotes erd
\end_inset

 file that reports all the profiling informations (the caller and callee
 graph, the number of calls of each functions, the spent time etc.).
\end_layout

\begin_layout Section
Approaches
\end_layout

\begin_layout Standard
According to the profiling, we decided to work on the do_step and do_step_dampin
g functions and to accelerate them in hardware.
\end_layout

\begin_layout Standard
Here the code of the do_step function:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "basicstyle={\scriptsize},breaklines=true,language=C,numbers=left,tabsize=2"
inline false
status open

\begin_layout Plain Layout

/*
\end_layout

\begin_layout Plain Layout

 * do_step - propagate wavefield one step in time
\end_layout

\begin_layout Plain Layout

 * p  - current wavefield
\end_layout

\begin_layout Plain Layout

 * pp - previous and next wavefield
\end_layout

\begin_layout Plain Layout

 * dvv - density (1) * velocity *velocity
\end_layout

\begin_layout Plain Layout

 */
\end_layout

\begin_layout Plain Layout

void do_step(float *__restrict p, float *__restrict pp, float *__restrict
 dvv, float *__restrict source_container){
\end_layout

\begin_layout Plain Layout

	int i3;  //Indexes
\end_layout

\begin_layout Plain Layout

	int n12=n1*n2;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

// #pragma omp parallel for
\end_layout

\begin_layout Plain Layout

	for(i3=ORDER; i3 < n3-ORDER; i3++){   //Loop over slowest axis
\end_layout

\begin_layout Plain Layout

		int i1;
\end_layout

\begin_layout Plain Layout

		int i2;
\end_layout

\begin_layout Plain Layout

		for(i2=ORDER; i2 < n2-ORDER; i2++){ //Loop over middle axis
\end_layout

\begin_layout Plain Layout

			for(i1=ORDER; i1< n1-ORDER; i1++){  //Loop over fast axis
\end_layout

\begin_layout Plain Layout

				//Wavefield update
\end_layout

\begin_layout Plain Layout

				pp[i1+i2*n1+i3*n12]=(2.0*p[i1+i2*n1+i3*n12]-pp[i1+i2*n1+i3*n12]+dvv[i1+i2*n1+
i3*n12]*(
\end_layout

\begin_layout Plain Layout

						p[i1+i2*n1+i3*n12]*c_0
\end_layout

\begin_layout Plain Layout

						+c_1[0]*(p[(i1+1)+(i2  )*n1+(i3  )*n12]+p[(i1-1)+(i2  )*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_1[1]*(p[(i1+2)+(i2  )*n1+(i3  )*n12]+p[(i1-2)+(i2  )*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_1[2]*(p[(i1+3)+(i2  )*n1+(i3  )*n12]+p[(i1-3)+(i2  )*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_1[3]*(p[(i1+4)+(i2  )*n1+(i3  )*n12]+p[(i1-4)+(i2  )*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_1[4]*(p[(i1+5)+(i2  )*n1+(i3  )*n12]+p[(i1-5)+(i2  )*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_2[0]*(p[(i1  )+(i2+1)*n1+(i3  )*n12]+p[(i1  )+(i2-1)*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_2[1]*(p[(i1  )+(i2+2)*n1+(i3  )*n12]+p[(i1  )+(i2-2)*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_2[2]*(p[(i1  )+(i2+3)*n1+(i3  )*n12]+p[(i1  )+(i2-3)*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_2[3]*(p[(i1  )+(i2+4)*n1+(i3  )*n12]+p[(i1  )+(i2-4)*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_2[4]*(p[(i1  )+(i2+5)*n1+(i3  )*n12]+p[(i1  )+(i2-5)*n1+(i3  )*n12])
\end_layout

\begin_layout Plain Layout

						+c_3[0]*(p[(i1  )+(i2  )*n1+(i3+1)*n12]+p[(i1  )+(i2  )*n1+(i3-1)*n12])
\end_layout

\begin_layout Plain Layout

						+c_3[1]*(p[(i1  )+(i2  )*n1+(i3+2)*n12]+p[(i1  )+(i2  )*n1+(i3-2)*n12])
\end_layout

\begin_layout Plain Layout

						+c_3[2]*(p[(i1  )+(i2  )*n1+(i3+3)*n12]+p[(i1  )+(i2  )*n1+(i3-3)*n12])
\end_layout

\begin_layout Plain Layout

						+c_3[3]*(p[(i1  )+(i2  )*n1+(i3+4)*n12]+p[(i1  )+(i2  )*n1+(i3-4)*n12])
\end_layout

\begin_layout Plain Layout

						+c_3[4]*(p[(i1  )+(i2  )*n1+(i3+5)*n12]+p[(i1  )+(i2  )*n1+(i3-5)*n12])
\end_layout

\begin_layout Plain Layout

						))+source_container[i1+i2*n1+i3*n12];
\end_layout

\begin_layout Plain Layout

			}
\end_layout

\begin_layout Plain Layout

		}
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

	return;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
The code of the do_step_damping function is fairly similar, the only difference
 is that before and after the assignment of a value to the array pp, we
 have this assignment:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,language=C,tabsize=3"
inline false
status open

\begin_layout Plain Layout

pp[i1+i2*n1+i3*n12]*=scale;
\end_layout

\end_inset


\end_layout

\begin_layout Standard
where the variable scale is computed at each interation of the most internal
 for.
\end_layout

\begin_layout Standard
So, these two sub-routine seemed the most suitable to be translated into
 maxeler java code since they were a series of operations repeated at each
 iteration and indipendent from the results just computed in that call,
 hence they could be easily pipelined by the machine.
\end_layout

\begin_layout Subsection*
First simple approaches
\end_layout

\begin_layout Standard
The first approaches were aimed at getting more confident with the Maxeler
 SLiC interface, Manager and Kernel.
 At the beginning, we used the Basic SLiC interface and our first approach
 consisted in computing, at each iteration of the most internal for, the
 value of pp on the DFE.
 The application, to compute each value of the pp array, uses a 3D cross
 centered in the point at offsets i1+i2*n1+i3*n12.
 Our idea was to copy, at each iteration, the values of the cross into an
 array and pass it as a stream to the DFE, which had simply to sum them
 and then return the value.
\end_layout

\begin_layout Standard
So, we created a stream containing the cross of elements, i.e., considering
 p[i1+i2*n1+i3*n12], the five previous and next elements of each dimension,
 and the values of arrays pp, dvv, source_container, c_1, c_2, c_3, c_4
 and c_0.
 This approach, of course, was very trivial since we were using the Maxeler
 platform only for computing a value at each for iteration, just like a
 C sub-routine, instead of exploiting the potentiality and the nature of
 the machine, hence we decided to drop it without sythentizing it.
 Indeed, we would have spent more time in creating the stream and getting
 the output, through PCI Express, than computing the value.
\end_layout

\begin_layout Standard
Another possible approach, developed by other group working on Maxeler RTM,
 was passing the arrays p, pp, dvv, source_container as streams to the DFE
 and using the same offsets that we would use in the C code to compute the
 output stream (an approach very similar to the C code).
 However, this was not feasible since the space needed to keep in BRAM (the
 memory on board, usually sized into kb) all the values required to compute
 a single output value was to much.
 Indeed, the more the offsets and the data get bigger the more space on
 BRAM is required.
\end_layout

\begin_layout Standard
So, we moved to two new different approaches: one with LMEM Blocked 3D,
 the other with Stalling Streams.
\end_layout

\begin_layout Subsection*
LMEM Blocked 3D
\end_layout

\begin_layout Standard
The DFE has two types of memory: FMem (Fast Memory) which can store several
 megabytes of data on-chip with terabytes/second of access bandwidth and
 LMem (Large Memory) which can store many gigabytes of data off-chip.
 In this approach, we used the dynamic SLiC interface to have complete managemen
t of the communication between C code and Manager.
 Our idea was to load in LMEM the array p as a 3D block.
 In this way, it would be possible to access to all its subcube.
 As it can be seen by looking at the code of the do_step, at each iteration,
 the computation of a value of pp required a 3D cross of dimension 11*11*11
 (stencil size).
 In this way, we thought we could access every subcube of p of dimension
 11*11*11 and compute the value of pp[i1+i2*n1*i3*n12].
 This approach would avoid to fill the BRAM since we would load only the
 values required by that computation without using big offsets.
 In addiction, we wanted to create several streams containing only the values
 of pp, dvv, source_container required to compute the output stream, that
 would be stored in LMEM.
 After the computation, we would read the output from LMEM and stored in
 the pp array.
 So, we can see this approach as a combination of the previous approaches,
 in a way.
 At the beginning, we thought we could move the subcube from the Kernel
 itself, but this was not possible.
 The only chance to do this was to pass through SLiC interface the offsets
 of the starting points of the 3D subcube from the C code.
 However, we begun to have several problems due to Maxeler machine architecture.
 In the manual, we found the function used by the Manager to select a subcube
 of the 3D block and pass it to the Kernel.
 This is the function:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,language=Java,tabsize=3"
inline false
status open

\begin_layout Plain Layout

public void setLMemBlocked(String streamName, long address,
\end_layout

\begin_layout Plain Layout

long arraySizeFast, long arraySizeMed, long arraySizeSlow, long rwSizeFast,
 long rwSizeMed, long rwSizeSlow, long offsetFast, long offsetMed, long
 offsetSlow)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
where the first three parameters are refered to the size 3D block in LMEM,
 the next three to the size of the subcube we want to consider, the last
 three are the offsets the subcube starts from.
\end_layout

\begin_layout Standard
This function was neither completely explained in the manual nor used in
 examples, so, before applying it to the RTM application, we modified an
 example in Maxide and made some tests on it.
 It turned out that this function wanted at least one of the three size
 of the 3D block in LMEM multiple of the Maxeler burst, in order to have
 it burst-aligned.
 The burst of Maxeler machine is 384 byte, hence we had to load a block
 of size x*y*z, where at least one of these three parameter had to be multiple
 of 96 (indeed, we used arrays of 32-bit float, so 4 byte per float).
 The size of stream p, like the size of streams pp, dvv, source_container,
 is given by the test files of the RTM application and none of its three
 dimensions are multiple of 96, thus we had to modify one of these dimensions
 (the fast one) and make it multiple of 96.
 However, in this way we added useless data to the streams but it was still
 possible to extract only the data we needed at the end of the computation.
 Another problem occured when we found out that not only one of the 3D block
 dimensions had to be multiple of 96, but also one of the dimensions of
 the subcube.
 This fact made our solution more complicated since we couldn't take subcubse
 of size 11*11*11 anymore, but we had to manage bigger subcubes.
 A possible solution was to compute from this new subcube as many useful
 values as we could and then move the offsets from the C code to compute
 the following values.
 For instance, if we were using the rtm_parameters_small.txt, the size of
 the arrays would be 100*100*100, then it would be modified to 192*100*100.
 The first subcube would start from offsets [0, 0, 0] and would be of size
 96*11*11.
 The first execution of Kernel would produce 86 useful values (from [5,5,5]
 to [91,5,5]).
 Then, we would move the offsets to [92,0,0].
 In fact, the only useful values we would need were the values at offsets
 [93,5,5], [94,5,5], [95,5,5], the others were useless.
 However, at this point the last and most critical problem rose: even at
 least one of the offsets had to be multiple of 96.
 As consequence, we were unable to exploit this approach since it was impossible
 to access to subcubes starting from offsets x,y,z where none of them was
 multiple of 96.
\end_layout

\begin_layout Subsection*
Stalling Streams
\end_layout

\begin_layout Standard
Our last approach was about stalling streams.
 The general idea behind this approach is: we have an array (p) and we access
 it as it was a 3D array, using three different offsets, so we can be linearized
 in three other arrays by following the dimensions x, y, z and then use
 them to compute the values of the output.
 Of course, the main issue of this approach is to synchronize the three
 arrays and make them flow asynchronously.
 In fact, we decided to have only of array linearized in one dimension (z),
 then move the 3D cross in that direction and collect the elements on it
 along the other two dimensions in the remaining arrays.
 
\end_layout

\begin_layout Standard
We designed an algorithm based on three streams: the first one was an array
 linearized by z dimension (p itself), the other two were arrays filled,
 respectively, with 10 elements in x and y dimensions necessary to compute
 each value of pp.
 To do so, we created at each execution of do_step and do_step_damping two
 new arrays: px and py.
 Each one of these contained n1*n2*n3*11 elements (to avoid mistakes by
 skipping the central value already in p).
 Because of the different size, we needed to find a method to synchronize
 these two arrays with the array p.
 Fortunately, Maxeler gives the opportunity to stall not only input streams
 but also output streams.
 Therefore, we created a controller of size n1*n2*n3*11 and filled it with
 blocks of one 1 and ten 0.
 The idea is: when the input/ouput stream on the Kernel has the 1, it can
 use/save the actual value and then to the next one.
 When it has 0, it can't go further.
 So, the streams p, pp, dvv, source_container would be driven by the controller,
 instead streams px and py would be free to move on at each tick.
 Moreover, there is no need to load streams in LMEM since the number of
 elements required to compute each output value was limited.
 Indeed, only when we have 1 on the controller we write the value on the
 output by using the 10 elements of px and py, the 11 elements of p and
 elements from the other streams, not so much space was required in BRAM.
 Of course, one of the drawbacks was the fact that both px and py contained
 duplicated data but it was the simpliest way to create the two streams.
 Another problem came from Maxeler because it didn't accept that a stream
 (pp) was both an input and output stream, hence we created a new output
 stream, ppresult, and then, at the end of the do_step/do_step_damping,
 we shifted the pointers of ppresult and pp.
 Moreover, we had to deal with the time of creation of px and py: both were
 created in three nested for, in total (n1-10)*(n2-10)*(n3-10) iterations
 (the same number of iterations of the original C code).
 Our first implementation of this algorithm was quite trivial, since we
 used the DFE only in the do_step sub-routine.
 At each call of the do_step we allocated the DFE (100msec to 1sec) and
 passed the 7 input streams (p, pp, dvv, source_container, px, py, controller),
 one output stream (ppresult) and several scalar values (c_0, c_1[], c_2[],
 c_3[], c_4[]).
 The Kernel received these streams and, when controller was 1, it computed
 the value of ppresult by using almost the same formula of C code.
 
\end_layout

\begin_layout Standard
Here we can se the Manager, the Kernel, and the do_step sub-routine using
 SLiC functions:
\end_layout

\begin_layout Standard
Manager:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,language=Java,numbers=left,tabsize=3"
inline false
status open

\begin_layout Plain Layout

public class CpuMainManager{
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	private static final String s_kernel1 = "linearKernel";
\end_layout

\begin_layout Plain Layout

	
\end_layout

\begin_layout Plain Layout

    public static void main(String[] args) {
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

        /*...*/
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

        //link of streams for CPU to DFE
\end_layout

\begin_layout Plain Layout

        manager.setIO(
\end_layout

\begin_layout Plain Layout

				link("p", IODestination.CPU),
\end_layout

\begin_layout Plain Layout

                /*...*/
\end_layout

\begin_layout Plain Layout

				);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		/*...*/
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	private static EngineInterface interfaceDefault() {
\end_layout

\begin_layout Plain Layout

		EngineInterface engine_interface = new EngineInterface();
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

        /*...*/
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		//creation of InterfaceParam
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

        InterfaceParam  c_1_0    = engine_interface.addParam("c_1_0", CPUTypes.DOU
BLE);
\end_layout

\begin_layout Plain Layout

        
\end_layout

\begin_layout Plain Layout

        /*...*/
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		//set of Kernel ticks
\end_layout

\begin_layout Plain Layout

        
\end_layout

\begin_layout Plain Layout

        engine_interface.setTicks(s_kernel1, sizeController);
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

        //set of InterfaceParam as streams and scalars
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

        engine_interface.setStream("p", type, size*sizeFloat);
\end_layout

\begin_layout Plain Layout

        engine_interface.setScalar(s_kernel1, "c_0", c_0);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

        /*...*/
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

        engine_interface.ignoreAll(Direction.IN_OUT);
\end_layout

\begin_layout Plain Layout

		return engine_interface;
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Kernel:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,language=Java,numbers=left,tabsize=3"
inline false
status open

\begin_layout Plain Layout

class CpuMainKernel extends Kernel {
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    protected CpuMainKernel(KernelParameters parameters) {
\end_layout

\begin_layout Plain Layout

		super(parameters);
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

        //creation of DFEVar
\end_layout

\begin_layout Plain Layout

        
\end_layout

\begin_layout Plain Layout

        DFEVar controller = io.input("controller", typeInt);
\end_layout

\begin_layout Plain Layout

		DFEVar p = io.input("p", typeFloat, controller.cast(dfeBool()));
\end_layout

\begin_layout Plain Layout

        
\end_layout

\begin_layout Plain Layout

        /*...*/
\end_layout

\begin_layout Plain Layout

		
\end_layout

\begin_layout Plain Layout

        DFEVar result= 2.0*p-pp+dvv*(
\end_layout

\begin_layout Plain Layout

				p*c_0
\end_layout

\begin_layout Plain Layout

				+c_1_0*(stream.offset(p, 1)+stream.offset(p,-1))
\end_layout

\begin_layout Plain Layout

				+c_1_1*(stream.offset(p, 2)+stream.offset(p,-2))
\end_layout

\begin_layout Plain Layout

				+c_1_2*(stream.offset(p, 3)+stream.offset(p,-3))
\end_layout

\begin_layout Plain Layout

				+c_1_3*(stream.offset(p, 4)+stream.offset(p,-4))
\end_layout

\begin_layout Plain Layout

				+c_1_4*(stream.offset(p, 5)+stream.offset(p,-5))
\end_layout

\begin_layout Plain Layout

				+c_2_0*(stream.offset(py, -4) + stream.offset(py, -6))
\end_layout

\begin_layout Plain Layout

				+c_2_1*(stream.offset(py, -3) + stream.offset(py, -7))
\end_layout

\begin_layout Plain Layout

				+c_2_2*(stream.offset(py, -2) + stream.offset(py, -8))
\end_layout

\begin_layout Plain Layout

				+c_2_3*(stream.offset(py, -1) + stream.offset(py, -9))
\end_layout

\begin_layout Plain Layout

				+c_2_4*(py + stream.offset(py, -10))
\end_layout

\begin_layout Plain Layout

				+c_3_0*(stream.offset(px, -4) + stream.offset(px, -6))
\end_layout

\begin_layout Plain Layout

				+c_3_1*(stream.offset(px, -3) + stream.offset(px, -7))
\end_layout

\begin_layout Plain Layout

				+c_3_2*(stream.offset(px, -2) + stream.offset(px, -8))
\end_layout

\begin_layout Plain Layout

				+c_3_3*(stream.offset(px, -1) + stream.offset(px, -9))
\end_layout

\begin_layout Plain Layout

				+c_3_4*(px + stream.offset(px, -10))
\end_layout

\begin_layout Plain Layout

				)+source_container;
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

		io.output("ppresult", result, typeFloat, controller.cast(dfeBool()));
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
do_step:
\end_layout

\begin_layout Standard
\begin_inset listings
lstparams "breaklines=true,language=C,numbers=left,tabsize=3"
inline false
status open

\begin_layout Plain Layout

void do_step(float *__restrict p, float *__restrict pp, float *__restrict
 dvv,
\end_layout

\begin_layout Plain Layout

		float *__restrict source_container) {
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    //variables creation
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    float *px;
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    /*...*/
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    //several data allocatations
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

	uint32_t *controller = malloc(size * stencilSize * sizeof(uint32_t));
\end_layout

\begin_layout Plain Layout

	ppresult = malloc(sizeBytes);
\end_layout

\begin_layout Plain Layout

	px = malloc(sizepxy);
\end_layout

\begin_layout Plain Layout

	py = malloc(sizepxy);
\end_layout

\begin_layout Plain Layout

 
\end_layout

\begin_layout Plain Layout

    /*...*/
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	for (i3 = ORDER; i3 < n3 - ORDER; i3++) { //Loop over slowest axis
\end_layout

\begin_layout Plain Layout

		for (i2 = ORDER; i2 < n2 - ORDER; i2++) { //Loop over middle axis
\end_layout

\begin_layout Plain Layout

			for (i1 = ORDER; i1 < n1 - ORDER; i1++, index++) {
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

                //assignment of values to px and py
\end_layout

\begin_layout Plain Layout

				px[((i1) + (i2) * n1 + (i3) * n12) * stencilSize - 10] = p[(i1)
\end_layout

\begin_layout Plain Layout

						+ (i2) * n1 + (i3 - 5) * n12];
\end_layout

\begin_layout Plain Layout

				py[((i1) + (i2) * n1 + (i3) * n12) * stencilSize - 10] = p[(i1)
\end_layout

\begin_layout Plain Layout

						+ (i2 - 5) * n1 + (i3) * n12];
\end_layout

\begin_layout Plain Layout

                
\end_layout

\begin_layout Plain Layout

                /*...*/
\end_layout

\begin_layout Plain Layout

			}
\end_layout

\begin_layout Plain Layout

		}
\end_layout

\begin_layout Plain Layout

	}
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	max_file_t *maxfile = CpuMain_init();
\end_layout

\begin_layout Plain Layout

	max_engine_t *engine = max_load(maxfile, "*");
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	max_actions_t* act = max_actions_init(maxfile, "default");
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

    //loading streams and scalars
\end_layout

\begin_layout Plain Layout

	max_queue_input(act, "p", p, size * sizeof(float));
\end_layout

\begin_layout Plain Layout

	max_queue_output(act, "ppresult", ppresult, size * sizeof(float));
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	max_set_param_double(act, "c_0", (double) c_0);
\end_layout

\begin_layout Plain Layout

    
\end_layout

\begin_layout Plain Layout

    /*...*/
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	//running engine
\end_layout

\begin_layout Plain Layout

	max_run(engine, act);
\end_layout

\begin_layout Plain Layout

	max_unload(engine);
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	/*...*/
\end_layout

\begin_layout Plain Layout

\end_layout

\begin_layout Plain Layout

	return;
\end_layout

\begin_layout Plain Layout

}
\end_layout

\end_inset


\end_layout

\begin_layout Standard
After several tests in simulation, we decided to synthetize the Kernel on
 the FPGA.
 Once the process was completed, we had these results regarding the resource
 usage:
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="left" valignment="top" width="0">
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Logic utilization:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LUTs:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Primary FFs:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Secondary FFs:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Multipliers (25x18):
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 / 2016
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
DSP blocks:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 / 2016
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Block memory (BRAM18):
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
 / 2128
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(%)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
Then, we run it to see its performance.
 It turned out that, using the file rtm_parameters_small.txt, the original
 RTM application run on Intel i7 processor of Maxeler, without pragma, took
 nearly 2 minutes, while the RTM application accelerated in hardware took
 26 minutes.
 We expected this result since our implementation was not optimized, this
 run was only useful as proof of the termination of our application.
 So, we begun to optimized the code by adding/editing several parts of it.
 Here the list of optimizations we implemented:
\end_layout

\begin_layout Itemize
the DFE is allocated only one time, then it's deallocated before the end
 of the main;
\end_layout

\begin_layout Itemize
the do_step_damping sub-routine is accelerated in hardware too.
 To do so, we created a new array containing the values of variable scale
 generated during the execution of do_step_damping;
\end_layout

\begin_layout Itemize
editing of the kernel to make it common for both do_step and do_step_damping
 (added the stream scale);
\end_layout

\begin_layout Itemize
the arrays px, py, scale, controller are allocated once with the DFE;
\end_layout

\begin_layout Itemize
since it never changes, the controller is loaded in the LMEM of the FPGA
 one time only.
\end_layout

\begin_layout Standard
After these changes, we were ready to synthetize this new Kernel, here the
 results of the resource usage:
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\begin_inset Tabular
<lyxtabular version="3" rows="7" columns="3">
<features tabularvalignment="middle">
<column alignment="left" valignment="top" width="0">
<column alignment="center" valignment="top" width="0">
<column alignment="left" valignment="top" width="0">
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Logic utilization:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
73339 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(24.64%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
LUTs:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
50815 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(17.07%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Primary FFs:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
61121 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(20.54%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Secondary FFs:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
11365 / 297600
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(3.82%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Multipliers (25x18):
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
28 / 2016
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1.88%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
DSP blocks:
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
38 / 2016
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(1.88%)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Block memory (BRAM18):
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" topline="true" bottomline="true" leftline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
252 / 2128
\end_layout

\end_inset
</cell>
<cell alignment="left" valignment="top" topline="true" bottomline="true" leftline="true" rightline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
(11.84%)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\end_layout

\begin_layout Plain Layout
\begin_inset Caption

\begin_layout Plain Layout

\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The performance of our application improved but it was still slower than
 the original RTM application.
 Indeed, now it took nearly 10 minutes to complete.
 So, for sure, there were some bottlenecks inside our code.
 The most probable bottlenecks where two: the creation of arrays px and
 py and the stream of data from the RAM of i7 to the FPGA via PCI Express.
 To find out the problem, we made several tests to analyze the time of creation
 the array px/py and the time of streaming data/computing the data on DFE.
 It turned out that the creation of the arrays took, on average, nearly
 0.014s, while the streaming/computation nearly 0.20s.
 As we thought, the streaming of data was the real bottleneck.
 
\end_layout

\begin_layout Standard
tempo trasferimento, ottimizzazioni
\end_layout

\begin_layout Section
Final Notes
\end_layout

\end_body
\end_document
